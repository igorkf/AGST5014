---
title: "Homework 2"
author: "Igor Kuivjogi Fernandes"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1 An ANOVA output is shown below. Fill in the missing information.       
One-way ANOVA      
```{r, echo=FALSE, results='asis'}
table <- '
| Source | DF | SS     | MS | F | P |
|--------|----|--------|----|---|---|
| Factor | 3  | 36.15  | ?  | ? | ? |
| Error  | ?  | ?      | ?  |   |   |
| Total  | 19 | 196.04 |    |   |   |
'
cat(table)
```

Completing the cells we got:   
```{r, echo=FALSE, results='asis'}
table <- '
| Source | DF | SS     | MS         | F        | P         |
|--------|----|--------|------------|----------|-----------|
| Factor | 3  | 36.15  | 12.05      | 1.213418 | 0.3369274 |
| Error  | 16 | 158.89 | 9.930625   |          |           |
| Total  | 19 | 196.04 |            |          |           |
'
cat(table)
```
The p-value is the $P(F > 1.123418) = 0.3369274$, with $\text{df}_\text{factor} = 3$ and $\text{df}_\text{error} = 16$.

### 2 I belong to a golf club in my neighborhood. I divide the year into three golf seasons: summer (June–September), winter (November–March), and shoulder (October, April, and May). I believe that I play my best golf during the summer (because I have more time and the course isn’t crowded) and shoulder (because the course isn’t crowded) seasons and my worst golf is during the winter (because when all of the part-year residents show up, the course is crowded, play is slow, and I get frustrated). Data from the last year are shown in the following table.   

We can write a hypothesis test as follows:       
\begin{align*}
& H_0: \text{the golf performance in the seasons are equal} \\
& H_1: \text{at least one golf performance differs}
\end{align*}
```{r}
df <- data.frame(
  season = c(rep('summer', 10), rep('shoulder', 7), rep('winter', 8)),
  y = c(83, 85, 85, 87, 90, 88, 88, 84, 91, 90, 91, 87, 84,
        87, 85, 86, 83, 94, 91, 87, 85, 87, 91, 92, 86)
)

# show the data
cat('summer:', df[df$season == 'summer', 'y'], '\n',
    'shoulder:', df[df$season == 'shoulder', 'y'], '\n',
    'winter:', df[df$season == 'winter', 'y'], '\n\n')

# one-way ANOVA
df$season <- as.factor(df$season)
fit <- aov(y ~ season, data = df)
summary(fit)
```

a) Do the data indicate that my opinion is correct? Use alpha 0.05.   
No, we don't reject the null hypothesis that the golf performance is equal, because $\text{p-value} = 0.144 > \alpha$, which means that the golf performance was the same for all the seasons.    

b) Analyze the residuals from this experiment and comment on model adequacy.
```{r, fig.asp = 1.1, fig.width = 5.3, fig.align = 'center'}
par(mfrow = c(2, 2))
plot(fit, which = 1)
plot(fit, which = 2)
plot(residuals(fit) ~ rownames(df), main = 'Residuals vs Exp. Unit', 
     font.main = 1, data = df, xlab = 'Experimental unit', ylab = 'Residuals')
abline(h = 0, lty = 2)
```
The Residual vs Fitted plot shows homoscedasticity, i.e., the variance is constant along the predicted values, and are equally distributed around zero.   
The Normal Q-Q plot shows that the residuals are normally distributed because the points are very aligned with the Q-Q line.  
The Residual vs Exp. Unit plot presents an constant variance across the different experimental units.    
In general, the residual plots represent a good model adequacy.   

### 3 An article in Environment International (Vol. 18, No. 4, 1992) describes an experiment in which the amount of radon released in showers was investigated. Radon-enriched water was used in the experiment, and six different orifice diameters were tested in shower heads. The data from the experiment are shown in the following table: 
```{r}
df <- data.frame(
  diameter = c(0.37, 0.51, 0.71, 1.02, 1.40, 1.99),
  rep1 = c(80, 75, 74, 67, 62, 60),
  rep2 = c(83, 75, 73, 72, 62, 61),
  rep3 = c(83, 79, 76, 74, 67, 64),
  rep4 = c(85, 79, 77, 74, 69, 66)
)
df
```

a) Does the size of the orifice affect the mean percentage of radon released? Use alpha 0.05.      
```{r}
# reshaping
df_long <- reshape(df, direction = 'long', idvar = 'diameter', varying = list(2:5), 
                   timevar = 'rep', v.names = 'radon')
df_long$diameter <- as.factor(df_long$diameter)  # use factor here to obtain 5 df!
rownames(df_long) <- NULL

# one-way ANOVA
fit <- aov(radon ~ diameter, data = df_long)
summary(fit)
```
Yes, the orifice diameter affects the mean percentage of radon because the $\text{p-value} = 3.16 \times 10^8 < \alpha$, which means we reject the null hypothesis that the means are equal.   

b) Find the P-value for the F statistic in part (a).   
```{r}
# it is the P(F > 30.85), where F is MS_factor / MS_error
# df1 is the DF of factor, and df2 is the DF of the errors
# lower.tail=F means we want the right side region of the quantile
pf(q = 30.85, df1 = 5, df2 = 18, lower.tail = F) 
```

c) Analyze the residuals from this experiment.   
```{r, fig.asp = 1.1, fig.width = 5.3, fig.align = 'center'}
par(mfrow = c(2, 2))
plot(fit, which = 5)
plot(fit, which = 1)
plot(fit, which = 2)
plot(residuals(fit) ~ rownames(df_long), main = 'Residuals vs Exp. Unit', 
     font.main = 1, data = df_long, xlab = 'Experimental unit', ylab = 'Residuals')
abline(h = 0, lty = 2)
```
The Residuals vs Factor Levels plot shows that some factor levels have more variability than others.   
The Residuals vs Fitted plot shows the residuals are somewhat homogeneous across the predicted values around around the zero.    
The Normal Q-Q shows a deviation from the Q-Q line, which means the normality of residuals is not met.   
Finally, the Residuals vs Exp. Unit plot does not show a horizontal pattern.   
In general, the residual plots don't show a good model adequacy.   

d) Construct a graph to compare the treatment means. What conclusions can you draw?   
```{r}
boxplot(radon ~ diameter, data = df_long, xlab = 'Orifice diameter', ylab = 'Radon (%)')
means <- tapply(df_long$radon, df_long$diameter, mean)
points(means, col = 'red', pch = 16)
```
The mean percentage in radon (red points) differs from each different treatment (orifice diameter), although some treatments show more similar radon percentage distribution (e.g. 0.51 and 0.7; 1.4 and 1.99). Only looking through this data we could be suspicious that the treatment means were significant.    

### 4 Suppose that we have data on the weight loss of 100 people, each person assigned to one specific diet, each diet having assigned to it the same number of people. In performing an ANOVA, the analysts arrived at the table below, which is incomplete; Fill in the blanks.
```{r, echo=FALSE, results='asis'}
table <- '
| Source of variability | SSQ | df | MSQ | Fcalc |
|-----------------------|-----|----|-----|-------|
| Diet                  | ?   | 3  | ?   | 15    |
| Error                 | ?   | ?  | 600 |       |
| Total                 | ?   | ?  |     |       |
'
cat(table)
```

We have $100$ people, so the Total $\text{df}$ is $n - 1 = 99$.    
$SSQ_E = \text{df}_E \times MSQ_E = 57600$.   
$F_{calc} = \frac{MSQ_F}{MSQ_E}$, so $MSQ_F = F_{calc} \times MSQ_E = 9000$.    
$SSQ_F = \text{df}_F \times MSQ_F = 27000$.   
```{r, echo=FALSE, results='asis'}
table <- '
| Source of variability | SSQ    | df | MSQ  | Fcalc |
|-----------------------|------- |----|------|-------|
| Diet                  | 27000  | 3  | 9000 | 15    |
| Error                 | 57600  | 96 | 600  |       |
| Total                 | 84600  | 99 |      |       |
'
cat(table)
```

### 5 Consider the data set HW2_Q5.csv, which represents the yield of soybean (in kg) grown using different potassium concentrations (in ppm). Are there significant differences due to the concentration of potassium used? Use alpha = 0.05.    
```{r}
df <- read.csv('HW2_Q5.csv')
df$dose <- as.factor(df$dose)
str(df)
```

```{r}
fit <- aov(yield ~ dose, data = df)
summary(fit)
```
We reject the null hypothesis that the mean yield of soybeans are equal, i.e., at least one mean yield differs, because the $\text{p-value} < 0.05$.   

### 6 Using information from question 4, write the statistical model (all 3: the cell means, the treatment effect, as well as the matrix form) explaining what each term means.    

#### Cell means model    
$$
Y_{ij} = \mu_i + \epsilon_{ij},
$$
where $Y_{ij}$ is the weight loss for the j-th experimental unit subject to the i-th diet, $i = 1, \dots, 4$, $j = 1, \dots, r_i$, and $r_i$ is the number of experimental units or replications in the i-th diet. In the Question 4, all the diet have the same number of replications ($r_1 = r_2 = r_3 = r_4 = 25$). The $\mu_i$ is the mean within the i-th diet. The errors $\epsilon_{ij}$ are i.i.d. normally distributed with mean $0$ and constant variance $\sigma^2$.    

#### Treatment effects model
$$
Y_{ij} = \mu + \tau_i + \varepsilon_{ij},
$$
where $\tau_i$ is the effect of the i-th diet and $\mu$ is the global mean. The errors $\epsilon_{ij}$ are i.i.d. normally distributed with mean $0$ and constant variance $\sigma^2$  

#### Matrix form model
$$
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
$$

where
$$
\boldsymbol{y} = \begin{pmatrix}
y_{1,1} \\
y_{1,2} \\
y_{1,3} \\
\vdots \\
y_{1,25} \\
y_{2,1} \\
y_{2,2} \\
y_{2,3} \\
\vdots \\
y_{2,25} \\
\vdots \\
y_{4,1} \\
y_{4,2} \\
y_{4,3} \\
\vdots \\
y_{4,25} \\
\end{pmatrix}_{100 \times 1}, \ \ 
%
\boldsymbol{X} = 
\begin{pmatrix}
1 & 1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 1 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 0 & 1 & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 0 & 0 & 0 & 1 \\
\end{pmatrix}_{100 \times 5}, \ \
%
\boldsymbol{\beta} = \begin{pmatrix}
\mu \\
\tau_{1} \\
\tau_{2} \\
\tau_{3} \\
\tau_{4} 
\end{pmatrix}_{5 \times 1}, \ \
%
\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_{1,1} \\
\epsilon_{1,2} \\
\epsilon_{1,3} \\
\vdots \\
\epsilon_{1,25} \\
\epsilon_{2,1} \\
\epsilon_{2,2} \\
\epsilon_{2,3} \\
\vdots \\
\epsilon_{2,25} \\
\vdots \\
\epsilon_{4,1} \\
\epsilon_{4,2} \\
\epsilon_{4,3} \\
\vdots \\
\epsilon_{4,25} \\
\end{pmatrix}_{100 \times 1}
$$
where $\boldsymbol{\epsilon} \sim \text{MVN}(\boldsymbol{0}, \sigma^2 \boldsymbol{I})$, where $\boldsymbol{0}$ is a vector of zeros, and $\boldsymbol{I}$ is an identity matrix.

### 7 In an experiment that aims to compare plant growth under the application of  3 different fertilizers, the researcher has 6 experimental units available. Assuming a CRD will be used with an equal number of replications, provide below the Linear Model in matrix form and write all matrices with their elements (see slide 5 from Week3_part1).

The experiment has $3$ treatments and $6$ experimental units. As the experiment used an equal number of replications, then there are $2$ replications per treatment:
$$
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
$$

where
$$
\boldsymbol{y} = \begin{pmatrix}
y_{11} \\
y_{12} \\
y_{21} \\
y_{22} \\
y_{31} \\
y_{32} 
\end{pmatrix}_{6 \times 1}, \ \ 
%
\boldsymbol{X} = 
\begin{pmatrix}
1 & 1 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 0 & 0 & 1  
\end{pmatrix}_{6 \times 4}, \ \
%
\boldsymbol{\beta} = \begin{pmatrix}
\mu \\
\tau_{1} \\
\tau_{2} \\
\tau_{3} 
\end{pmatrix}_{4 \times 1}, \ \
%
\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{pmatrix}_{6 \times 1}
$$

### 8 Design a CRD, i.e., create a table with the randomized treatments for the experiment mentioned in question 7. Include your table below and indicate how it was generated.
```{r}
set.seed(2023)
factors <- as.factor(rep(c('a', 'b', 'c'), 2))  # 3 treatments, 2 repetitions each
factors <- sample(factors)  # randomize treatments
eus <- seq(1:length(factors))  # the experimental units
df <- data.frame(eu = eus, treat = factors)
print(df, row.names = FALSE)
```

```{r}
# check that the number of repetitions for each treatment are equal
table(df$treat)
```

### 9 The data set HW2_Q9.csv is from an experiment aiming to evaluate the influence of yacon flour consumption on the glycemic index. Run an ANOVA, check the assumptions, and if they are not met, re-run the model with the appropriate transformation.   
```{r}
df <- read.csv('HW2_Q9.csv')
df$treat <- as.factor(df$treat)
str(df)
```

```{r}
fit1 <- aov(y ~ treat, data = df)
summary(fit1)
```

```{r, fig.asp = 1.1, fig.width = 5.3, fig.align = 'center'}
par(mfrow = c(2, 2))
plot(fit1, which = 5)
plot(fit1, which = 1)
plot(fit1, which = 2)
plot(residuals(fit1) ~ rownames(df), main = 'Residuals vs Exp. Unit', 
     font.main = 1, data = df, xlab = 'Experimental unit', ylab = 'Residuals')
abline(h = 0, lty = 2)
```
The residuals variance is not homogeneous across the fitted values. Actually, the variance seems to increase as the fitted values increase. This also happens when we look at the residuals within each factor level. The normality assumption is not met. Moreover, the residuals across the experimental units does not show a horizontal pattern. 

```{r}
bc <- MASS::boxcox(fit1, data = df)  # do box-cox power transformation search
lambda <- bc$x[which.max(bc$y)]  # pick lambda with the highest log-likelihood
df$y_trans <- df$y ^ lambda  # transform the response variable

# fit again but using the transformed response variable
fit2 <- lm(y_trans ~ treat, data = df)
anova(fit2)
```

```{r, fig.asp = 1.1, fig.width = 5.3, fig.align = 'center'}
par(mfrow = c(2, 2))
plot(fit2, which = 5)
plot(fit2, which = 1)
plot(fit2, which = 2)
plot(residuals(fit2) ~ rownames(df), main = 'Residuals vs Exp. Unit', 
     font.main = 1, data = df, xlab = 'Experimental unit', ylab = 'Residuals')
abline(h = 0, lty = 2)
```
The residual plots are way better. The homoscedasticity is present, with residuals varying constantly around 0, and the normality seems to be met (with slightly deviations). The residuals also are constant between the different factor levels, and the residuals follow a horizontal pattern when looking across the experimental units.       

### 10 There is an "Exercises" section at the end of chapter 2. For this homework, answer question 5 of the Exercises list of chapter 2.   
The effect of plant growth regulators and spear bud scales on spear elongation in asparagus was investigated by Yang-Gyu and Woolley (2006).
Elongation rate of spears is an important factor determining final yield of asparagus in many temperate climatic conditions. 
Spears were harvested from 6-year-old Jersey Giant asparagus plants grown in a commercial planting at Bulls (latitude 40.2S, longitude 175.4E), New Zealand. 
Spears were harvested randomly and transported from field to lab for investigation. 
After trimming to 80mm length, spears were immersed completely for 1h inaqueous solutions of 10 mg l-1 concentration of indole-3-acetic acid (IAA), abscisic acid (ABA), GA3, or CPPU (Sitofex EC 2.0%; SKW, Trostberg, Germany) in test tubes. Control spears were submerged in distilled water for 1h. 
The experiment was a completely randomized design with five replications (spears) per treatment. The resulting data (final spear length in mm) is shown below.      
```{r, echo=FALSE, results='asis'}
table <- '
|Control | IAA | ABA  | GA3  | CPPU |
|--------|-----|------|------|------|
|94.7    |89.9 | 96.8 | 99.1 |104.4 |
|96.1    |94.0 | 87.8 | 95.3 |98.9  |    
|86.5    |99.1 | 89.1 | 94.6 |98.9  |
|98.5    |92.8 | 91.1 | 93.1 |106.5 |
|94.9    |99.4 | 89.4 | 95.7 |104.8 |
'
cat(table)
```

a) Perform the analysis of variance to test the hypothesis of no treatment effect.   
```{r}
df <- data.frame(
  treat = c(rep('control', 5), rep('IAA', 5), rep('ABA', 5), rep('GA3', 5), rep('CPPU', 5)),
  spear_length = c(94.7, 96.1, 86.5, 98.5, 94.9, 
                   89.9, 94.0, 99.1, 92.8, 99.4,
                   96.8, 87.8, 89.1, 91.1, 89.4, 
                   99.1, 95.3, 94.6, 93.1, 95.7, 
                   104.4, 98.9, 98.9, 106.5, 104.8)
)

# control must be the reference level to properly interpret Dunnett test
df$treat <- factor(df$treat, levels = c('control', 'IAA', 'ABA', 'GA3', 'CPPU'))  

# one-way ANOVA
fit <- aov(spear_length ~ treat, data = df)
summary(fit)
```
We reject the null hypothesis that the treatment means are equal (at a significance level of $\alpha = 0.05$), because $\text{p-value} < 0.05$, which means that at least one treatment mean
is different.   

b) Use the Tukey method to test all pairwise comparisons of treatment means.  
```{r}
fit_tukey <- TukeyHSD(fit, ordered = TRUE)
fit_tukey
```
There are some significant differences (using a significant level of $\alpha = 0.05$): CPPU-ABA; CPPU-control; CPPU-IAA; CPPU-GA3, because all of them have a $\text{p-value} < 0.05$.
The other ones have not significant differences.   

c) Use the Dunnett procedure to compare all treatment group means to the control mean.
```{r, message = F}
library(multcomp)
fit_dunnett <- glht(fit, linfct = mcp(treat = 'Dunnett'), alternative = 'two.sided')
summary(fit_dunnett)
```

There's only one significant difference (using a significant level of $\alpha = 0.05$): CPPU-control, which is the same significant difference Tukey HSD test found regarding only control based differences.   



